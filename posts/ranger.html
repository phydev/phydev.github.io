<!--{"title": "A cautionary tale on imputation with random forest", "date": "10-08-2023", "title_en": "A cautionary
tale on imputation with random forest", "status": "show", "order": "2"} -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://code.jquery.com/jquery-1.10.2.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <base href="https://phydev.github.io/">
    <script>
        MathJax = {
            tex: {
                tags: 'all'  // should be 'ams', 'none', or 'all'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/scripts.js"></script>
    <link rel="stylesheet" href="/style.css">
    <style>
        table {
            border-collapse: collapse;
            width: 50%;
            margin: 0 auto;
            font-family: Arial, sans-serif;
        }
        
        th, td {
            border: none;
            padding: 8px;
            text-align: center;
        }
        
        th {
            background-color: #f2f2f2;
        }
        
        .top-bar, .middle-bar, .bottom-bar {
            background-color: #4CAF50;
            color: white;
            text-align: center;
            padding: 10px;
        }
        
        .middle-bar {
            background-color: #333;
            padding: 5px;
        }
        
        .bottom-bar {
            background-color: #4CAF50;
            padding: 10px;
        }
    </style>
    <title>Maurício Moreira-Soares</title>
</head>
<body>
<!--<iframe src="header.html" onload="this.before((this.contentDocument.body||this.contentDocument).children[0]);this.remove()"></iframe>-->

<!--Navigation bar-->
<div id="nav-placeholder">

</div>

<script>
    $(function () {
        $("#nav-placeholder").load("/header.html");
    });
</script>


<main id="main">
    <h1>A cautionary tale on imputation with random forest</h1>
    
    <h2>Missing data and imputation methods</h2>
    <p> Missing data is ubiquitous in data science and has the potential to significantly impact results if not handled
        properly. Historically, three primary mechanisms for missing data have been defined: </p>
    
    <ul>
        <li> Missing completely at random (MCAR): the probability of missingness is the same for all observations.</li>
        <li> Missing at random (MAR): the probability of missingness depends on observed variables.</li>
        <li> Missing not at random (MNAR): the probability of missingness depends on unobserved variables.
    
    </ul>
    
    <p>As a side note, with the rise of big data and increased interoperability, new types of missingness have emerged
       that do not fit into this traditional classification. One such type is
        <a href="https://www.nature.com/articles/s42256-022-00596-z#Bib1" target="_blank" rel="noopener noreferrer">structured
                                                                                                                    missingness</a><a
                class="footnote">&sup1<span>Mitra, R., McGough, S.F., Chakraborti, T. et al. Learning from data with structured missingness. Nat Mach Intell 5, 13–23 (2023). https://doi.org/10.1038/s42256-022-00596-z</span></a>,
       which occurs when datasets with different sets of variables are integrated.</p>
    
    <p>Several methods are available for handling missing data, each with its particular strengths and weaknesses, such
       as mean and median imputation, missing indicator method, regression imputation, among many others. In this study
       we will focus only on two methods, the gold standard
        <a href="https://www.jstatsoft.org/article/view/v045i03" target="_blank" rel="noopener noreferrer">
            multivariate imputation by chained equations (MICE)</a><a class="footnote">&sup2<span>Stef van Buuren,
                                                                                                  Karin Groothuis-Oudshoorn. mice: Multivariate Imputation by
                                     Chained Equations in R. Journal of Statistical, (2011).
                                                                                                  https://www.jstatsoft.org/article/view/v045i03</span></a>
       implemented
       in the R package "MICE", and the machine learning based method <i>fast imputation with Random Forest</i>
       <a href="https://github.com/farrellday/miceRanger" target="_blank" rel="noopener noreferrer">"miceRanger"
                                                                                                    package.</a></p>
    
    <p>Both methods featured here use multiple imputation, but they differ significantly in two aspects. First, MICE
       uses a linear regression model to impute missing values, while miceRanger uses a Random Forest (RF) model.
       Second,
       MICE uses Gibbs sampler with Markov Chain Monte Carlo (MCMC) for sampling the joint distribution, while
       miceRanger uses empirical cumulative distribution function (ECDF) for sampling. The latter is the more robust
       approach as it attempts to model the whole joint distribution of the data.</p>
    
    <p>The first aspect means that because of the nature of RF, miceRanger allows the introduction of
       non-linearities and agnostic interactions during the imputation process. This comes with the price of a larger
       sample size requirement for reducing overfitting and more hyperparameters to be tuned for the
       imputation. </p>
    
    <p>
        The aim of this analysis is: 1) compare the coefficient of determination \( R^2\) and 2)
        regression coefficient estimates \( \hat{\beta}_i\) after MICE and miceRanger
        imputation. We will verify if miceRanger can provide unbiased estimates for a linear regression model fitted
        on MCAR data and how it compares with classical MICE estimates.
    </p>
    
    <h2>Simulation</h2>
    
    <p>We generated three sets of 100 datasets, each containing 1000 data points, with covariates \( (x_1, x_2) \) drawn
       from a bivariate distribution of means \( \vec{\mu }= (1, 80) \), standard deviations \( \vec{\sigma} = (10, 30)
       \) and covariates correlation \( \rho = 0 \), giving the following variance-covariance matrix:</p>
    
    <p>
    <center> \( \Sigma = \begin{bmatrix} 100 & 0 \\ 0 & 900 \end{bmatrix} \)</center>
    </p>
    
    <p>The response is given by \( y \equiv \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon \) with \( \vec{\beta} =
       (0, 1, -0.3) \). We control the coefficient of determination \(R^2\) by tuning the amount of gaussian noise \(
       \varepsilon \sim \mathcal{N}(0,\,\sigma_{\varepsilon}^{2})\) in the model. The three complete sets of data
       provide linear models with \(\hat{R^2} = (0.2, 0.5, 0.8) \) respectively. In summary, the data points are
       generated according to a linear model with specified means, standard deviations, coefficients, and \( R^2 \)
       value. Fig. 1 (a) depicts the response and independent variable distributions and their relationship. </p>
        <p> We then apply amputation to introduce missing values in the simulated data using the MCAR mechanism. For each
        dataset we create three different versions with missing rates of \(0.2\), \(0.5\) and \(0.8\), accounting for
        900 datasets in total. In Fig. 2 (b) we show the missing pattern used to generate the amputed data, only one
        missing variable is allowed per observation. In Table 1 we summarise the different conditions explored in this
        study.</p>
     <div class="figure-container">
         (a)
        <img src="/posts/figs/pairplot.png" alt="Pairplot" class="center" style="width:40%">
         
         (b)
         <img src="/posts/figs/missing_pattern.png"
             alt="Missing pattern used in the data. Only one variable is missing per observation."
             class="center"
             style="width:40%">
    </div>
    <caption> Figure 1: (a) Pair plot showing the response
              and independent variables
              distributions. (b) Missing patterns used in the amputation. Only one missing variable per observation is
                  allowed.
        </caption>
    <p></p>
    

    
        
      
   
    <p>
    
          Table 1: Datasets with different missing rates and \(R^2\) values. In total we have 9 conditions
                    with 100 replicates each.
               <a href="javascript:void(0);" onclick="showHide('table1')">Show Table 1</a></p>
    <table id="table1"  style="display:none;">
 
        <tr>
            <th>Dataset ID</th>
            <th>Missing rate</th>
            <th>\(R^2 \)</th>
        </tr>
        <tr>
            <td>curie</td>
            <td>0.2</td>
            <td>0.2</td>
        </tr>
        <tr>
            <td>newton</td>
            <td>0.5</td>
            <td>0.2</td>
        </tr>
        <tr>
            <td>lovelace</td>
            <td>0.8</td>
            <td>0.2</td>
        </tr>
        <tr>
            <td>fermi</td>
            <td>0.2</td>
            <td>0.5</td>
        </tr>
        <tr>
            <td>dirac</td>
            <td>0.5</td>
            <td>0.5</td>
        </tr>
        <tr>
            <td>noether</td>
            <td>0.8</td>
            <td>0.5</td>
        </tr>
        <tr>
            <td>born</td>
            <td>0.2</td>
            <td>0.8</td>
        </tr>
        <tr>
            <td>oppenheimer</td>
            <td>0.5</td>
            <td>0.8</td>
        </tr>
        <tr>
            <td>feynman</td>
            <td>0.8</td>
            <td>0.8</td>
        </tr>
    </table>
    
    <h2>Results and discussion</h2>
    
    <p>The Figure 2 shows coefficient of determinations for different correlations and
       missingness degree estimated with linear regression after
    MICE and miceRanger imputation.
    For MICE, we see that the confidence intervals always cover the true value of \(R^2\) and the mean estimate is
        close to the true value independent of the strength of the correlation between the response and covariates,
       and the amount of missingness in the data. The confidence intervals are bigger for higher amount of
       missingness due to increased uncertainty, as expected.
    </p>
    
    <p> On the other hand, we can see that performances obtained with miceRanger are always overestimated, which
    indicates that the imputation method is introducing a stronger correlation than the true relationship in the
    data. This problem becomes exacerbated for high amount of missingness (50 and 75%) where the true
    value is not covered by the confidente interval.
    </p>
    
    
    

    
    
    <div class="figure-container">
        <img src="/posts/figs/mice_results.png" alt="Results with mice" class="center" style="width:40%">
    <img src="/posts/figs/ranger_results.png" alt="Results with miceRanger." class="center" style="width:40%">
    </div>
    <caption> Figure 2: Coefficient of determination for each of the 9 conditions and the 95% confidence
                  intervals after MICE imputation (left) and miceRanger (right). The true value is depicted by dashed
              lines.
        </caption>
    <p></p>

    
    <p>To investigate this problem further we run another simulation where there is no correlation between the response
       variable and the covariate. We trained Random Forest and linear regression models. Fig. 3 shows the predicted
       points along with the original data for both methods. We see that Random Forest overfitted to the training data
        and predicts values close to the original data, even though there is no correlation in the original data. The
       \(\hat{R^2}\) are \(0.80\) and \(0.16 \times 10^{-3}\) for RF and LR, respectively.
    </p>
    
    <p>Fig. 4 shows how estimated parameters are affected when data is imputed using MICE (left) and miceRanger (right)
       with data generated under intermediate correlation, R² = 0.50. For MICE, we see that the confidence interval
       becomes larger as the amount of missingness increases, but the true value remains covered. On the other hand,
       estimates from miceRanger exhibit a huge effect overestimation and small confidence intervals that do not cover
       the true value, indicating that the imputation method introduces a stronger correlation.
    </p>
    <img src="/posts/figs/overfitting.png"
                         alt="Shows two figures with predictions from Random Forest and linear regression on a dataset with no correlation."
                         class="center"
                         style="width:40%">
        <caption> Figure 3: The true data is depicted in red and the predictions in blue for Random Forest (left) and
                  linear regression (right).
        </caption>

     <p></p>
    

    
    
    <div class="figure-container">
        <figure class="figure">
            <img src="/posts/figs/mice_beta1_r2_05.png" alt="Figure 1">
           
        </figure>
        <figure class="figure">
            <img src="/posts/figs/ranger_beta1_r2_05.png" alt="Figure 2">
  
        </figure>
    </div>
    
     <p>Figure 4: Estimated parameter values obtained using MICE (left) and miceRanger (right) for data with
                  intermediate correlation \(R² = 0.50\). Dashed lines represent the true
                 value and CI estimated without missing values.</p>

    
    
    <h2>Conclusion</h2>
    
    <p>We argue that RF-based methods have the potential to overfit to previously seen data which is
       dangerous when applied to imputation
       in classical inference studies. New imputation methods developed with this class of estimators require careful
       evaluation w.r.t their impact on parameter estimation. We
       observed that spurious
       correlations
       and
       interactions can be
       introduced
       during the imputation phase, affecting results further in the analysis pipeline with high risk of bias. </p>
    <p>We strongly advocate for directing efforts towards developing theoretical guarantees for newly created ML-based
       imputation methods. It is essential that we bridge the gap between the rapid pace of tool development and the
       need for rigorous theoretical foundations, including statistical guarantees. By doing so, we can ensure the
       scientific community has confidence in the reliability and effectiveness of these innovative approaches,
       ultimately accelerating their adoption and impact.
    </p>
    
    <h2>References</h2>
<p>Mitra, R., McGough, S.F., Chakraborti, T. et al. Learning from data with structured missingness. Nat Mach
       Intell 5,
      13–23 (2023). https://doi.org/10.1038/s42256-022-00596-z</p>
    
    <p>
    Stef van Buuren, Karin Groothuis-Oudshoorn. mice: Multivariate Imputation by
                                     Chained Equations in R. Journal of Statistical, (2011).
    Software. https://www.jstatsoft.org/article/view/v045i03
    </p>
</main>


<div id="bottom-placeholder">

</div>

<script>
    $(function () {
        $("#bottom-placeholder").load("/bottom.html");
    });
</script>
</body>
</html>
